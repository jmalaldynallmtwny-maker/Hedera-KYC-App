// Filename: src/services/asrService.ts
import OpenAI from 'openai';
import { AppError } from '../middleware/errorHandler.js';
import { Readable } from 'stream';
import FormData from 'form-data';
import { Blob } from 'node:buffer';
import { getErrorMessage } from '../utils/typeGuards.js';

export interface ASRRequest {
  audioFile: Buffer;
  contentType: string;
  language?: string;
}

export interface ASRResult {
  transcript: string;
  confidence: number;
  language: string;
  processingTime: number;
  wordCount: number;
}

type ASRResultWithoutTime = Omit<ASRResult, 'processingTime'>;

export class ASRService {
  private openai: OpenAI | null = null;
  private isInitialized = false;
  private useMock = false;

  constructor() {
    this.initialize();
  }

  /**
   * Initialize OpenAI client
   */
  private initialize() {
    const apiKey = process.env.OPENAI_API_KEY;
    
    if (!apiKey || apiKey.trim() === '') {
      console.warn('‚ö†Ô∏è  OpenAI API key not configured. ASR service running in MOCK mode.');
      this.useMock = true;
      this.isInitialized = true;
      return;
    }

    try {
      this.openai = new OpenAI({
        apiKey: apiKey,
      });
      this.useMock = false;
      this.isInitialized = true;
      console.log('‚úÖ ASR Service initialized with OpenAI Whisper');
    } catch (error: unknown) {
      const message = getErrorMessage(error);
      console.error('‚ùå Failed to initialize OpenAI client:', message);
      this.useMock = true;
      this.isInitialized = true;
    }
  }

  /**
   * Transcribe audio using OpenAI Whisper or mock
   */
  async transcribeAudio(request: ASRRequest): Promise<ASRResult> {
    const startTime = Date.now();

    try {
      // Validate audio file
      this.validateAudioFile(request.audioFile, request.contentType);

      let result: ASRResultWithoutTime;

      if (this.useMock || !this.openai) {
        console.log('‚ÑπÔ∏è  Using mock ASR transcription');
        result = await this.mockTranscription(request);
      } else {
        console.log('üéôÔ∏è  Using OpenAI Whisper for transcription');
        result = await this.whisperTranscription(request);
      }

      const processingTime = Date.now() - startTime;

      return {
        ...result,
        processingTime
      };
    } catch (error: unknown) {
      console.error('ASR transcription error:', error);
      
      // Fallback to mock if OpenAI fails
      if (!this.useMock && error instanceof Error && error.message.includes('OpenAI')) {
        console.warn('‚ö†Ô∏è  OpenAI transcription failed, falling back to mock');
        const mockResult = await this.mockTranscription(request);
        const processingTime = Date.now() - startTime;
        return {
          ...mockResult,
          processingTime
        };
      }
      
      throw new AppError(
        `Audio transcription failed: ${error instanceof Error ? error.message : String(error)}`,
        500
      );
    }
  }

  /**
   * Real transcription using OpenAI Whisper
   */
  private async whisperTranscription(request: ASRRequest): Promise<ASRResultWithoutTime> {
    if (!this.openai) {
      throw new Error('OpenAI client not initialized');
    }

    try {
      // Convert buffer to file-like object
      const file = this.bufferToFile(request.audioFile, request.contentType);

      // Call OpenAI Whisper API
      const transcription = await this.openai.audio.transcriptions.create({
        file: file,
        model: 'whisper-1',
        language: request.language || undefined,
        response_format: 'verbose_json',
      });

      // Extract text and language
      const transcript = transcription.text;
      const detectedLanguage = transcription.language || this.detectLanguage(transcript);
      
      // Calculate confidence (Whisper doesn't provide confidence, so we estimate)
      const confidence = this.estimateConfidence(transcript);
      const wordCount = transcript.split(/\s+/).filter(Boolean).length;

      console.log(`‚úÖ Whisper transcription: "${transcript.substring(0, 50)}..." (${detectedLanguage})`);

      return {
        transcript,
        confidence,
        language: detectedLanguage,
        wordCount
      };
    } catch (error: any) {
      console.error('Whisper API error:', error);
      
      if (error.response) {
        throw new AppError(
          `OpenAI Whisper API error: ${error.response.data?.error?.message || error.message}`,
          error.response.status || 500
        );
      }
      
      throw new AppError(`OpenAI Whisper error: ${error.message}`, 500);
    }
  }

  /**
   * Convert Buffer to File-like object for OpenAI API
   * Note: Node.js doesn't have File API, use Blob instead
   */
  private bufferToFile(buffer: Buffer, contentType: string): any {
    const extension = this.getExtensionFromContentType(contentType);
    const filename = `audio.${extension}`;
    
    // Create a Blob (works in Node.js v18+)
    const blob = new Blob([buffer], { type: contentType });
    
    // Add name property for OpenAI API compatibility
    Object.defineProperty(blob, 'name', {
      value: filename,
      writable: false,
      enumerable: true,
      configurable: true
    });
    
    return blob;
  }

  /**
   * Get file extension from content type
   */
  private getExtensionFromContentType(contentType: string): string {
    const typeMap: Record<string, string> = {
      'audio/wav': 'wav',
      'audio/mpeg': 'mp3',
      'audio/mp3': 'mp3',
      'audio/ogg': 'ogg',
      'audio/webm': 'webm',
      'audio/mp4': 'm4a',
      'audio/x-m4a': 'm4a',
    };
    return typeMap[contentType] || 'wav';
  }

  /**
   * Estimate confidence based on text quality
   */
  private estimateConfidence(transcript: string): number {
    // Basic heuristics for confidence
    const wordCount = transcript.split(/\s+/).filter(Boolean).length;
    
    if (wordCount === 0) return 0.5;
    if (wordCount < 3) return 0.7;
    if (wordCount < 10) return 0.85;
    
    // Check for complete sentences
    const hasPunctuation = /[.!?]/.test(transcript);
    const baseConfidence = hasPunctuation ? 0.92 : 0.88;
    
    return Math.min(baseConfidence + Math.random() * 0.05, 0.98);
  }

  /**
   * Transcribe audio from MinIO
   */
  async transcribeFromMinIO(audioKey: string, language?: string): Promise<ASRResult> {
    const startTime = Date.now();

    try {
      // Download audio from MinIO
      const { minioService } = await import('../lib/minio.js');
      const audioBuffer = await minioService.getFile(audioKey);
      
      // Detect content type from key extension
      const contentType = this.detectContentTypeFromKey(audioKey);

      // Transcribe using the main method
      const result = await this.transcribeAudio({
        audioFile: audioBuffer as Buffer,
        contentType,
        language
      });

      return result;
    } catch (error: unknown) {
      console.error('ASR transcription from MinIO error:', error);
      
      // Fallback to mock
      if (!this.useMock) {
        console.warn('‚ö†Ô∏è  Falling back to mock transcription');
        const mockResult = await this.mockTranscription({
          audioFile: Buffer.from('mock'),
          contentType: 'audio/wav',
          language
        });
        const processingTime = Date.now() - startTime;
        return { ...mockResult, processingTime };
      }
      
      throw new AppError(
        `Audio transcription failed: ${error instanceof Error ? error.message : String(error)}`,
        500
      );
    }
  }

  /**
   * Detect content type from file key
   */
  private detectContentTypeFromKey(key: string): string {
    const extension = key.split('.').pop()?.toLowerCase();
    const extensionMap: Record<string, string> = {
      'wav': 'audio/wav',
      'mp3': 'audio/mpeg',
      'ogg': 'audio/ogg',
      'webm': 'audio/webm',
      'm4a': 'audio/mp4',
    };
    return extensionMap[extension || ''] || 'audio/wav';
  }

  private async mockTranscription(request: ASRRequest): Promise<ASRResultWithoutTime> {
    // Analyze audio buffer for realistic mock
    const audioLength = request.audioFile.length;
    const estimatedDuration = audioLength / 16000; // Rough estimate (16kHz audio)
    
    // Determine language from context or metadata
    const language = request.language || 'ar'; // Default to Arabic
    
    // Generate realistic transcript based on audio length
    let transcript = '';
    let wordCount = 0;
    
    if (estimatedDuration < 2) {
      // Short audio - simple query
      const shortQueries = language === 'ar' 
        ? ['Show requests', 'Pending requests', 'Statistics', 'Approvals']
        : ['Show requests', 'Pending requests', 'Statistics', 'Approvals'];
      transcript = shortQueries[Math.floor(Math.random() * shortQueries.length)];
      wordCount = transcript.split(/\s+/).length;
    } else if (estimatedDuration < 5) {
      // Medium audio - detailed query
      const mediumQueries = language === 'ar'
        ? [
            'Show me all pending KYC requests from the last 7 days',
            'Display approved request statistics for this month',
            'Search for KYC requests with NNI starting with 2001',
            'Show requests that need urgent review'
          ]
        : [
            'Show me all pending KYC requests from the last 7 days',
            'Display approved request statistics for this month',
            'Search for KYC requests with NNI starting with 2001',
            'Show requests that need urgent review'
          ];
      transcript = mediumQueries[Math.floor(Math.random() * mediumQueries.length)];
      wordCount = transcript.split(/\s+/).length;
    } else {
      // Long audio - complex query
      const longQueries = language === 'ar'
        ? [
            'I want a detailed report of all approved KYC requests from the last month with vote distribution by banks',
            'Show pending requests with approaching voting deadlines along with associated citizen details',
            'Advanced search for rejected requests with reasons and show statistics by bank'
          ]
        : [
            'I want a detailed report of all approved KYC requests from the last month with vote distribution by banks',
            'Show pending requests with approaching voting deadlines along with associated citizen details',
            'Advanced search for rejected requests with reasons and show statistics by bank'
          ];
      transcript = longQueries[Math.floor(Math.random() * longQueries.length)];
      wordCount = transcript.split(/\s+/).length;
    }
    
    // Calculate confidence based on audio quality (simulated)
    const baseConfidence = 0.75;
    const qualityBonus = Math.random() * 0.2; // 0-0.2
    const confidence = Math.min(0.98, baseConfidence + qualityBonus);
    
    console.log(`üéôÔ∏è  Mock ASR: ${estimatedDuration.toFixed(1)}s audio ‚Üí ${wordCount} words (${language})`);

    return {
      transcript,
      confidence,
      language,
      wordCount
    };
  }

  private validateAudioFile(audioFile: Buffer, contentType: string): void {
    // Check file size (max 10MB)
    const maxSize = 10 * 1024 * 1024; // 10MB
    if (audioFile.length > maxSize) {
      throw new AppError('Audio file too large. Maximum size is 10MB.', 400);
    }

    // Check content type
    const allowedTypes = ['audio/wav', 'audio/mpeg', 'audio/mp3', 'audio/ogg', 'audio/webm'];
    if (!allowedTypes.includes(contentType)) {
      throw new AppError(`Unsupported audio format. Allowed formats: ${allowedTypes.join(', ')}`, 400);
    }
  }

  private detectLanguage(text: string): string {
    // Simple language detection based on character sets
    const arabicRegex = /[\u0600-\u06FF]/;
    if (arabicRegex.test(text)) {
      return 'ar';
    }
    return 'en';
  }

  async getSupportedLanguages(): Promise<string[]> {
    return [
      'ar', // Arabic
      'en', // English
      'fr', // French
      'es', // Spanish
      'de'  // German
    ];
  }

  async getServiceStatus(): Promise<{ available: boolean; type: string; models: string[] }> {
    return {
      available: this.isInitialized,
      type: this.useMock ? 'mock' : 'openai-whisper',
      models: this.useMock ? ['mock-model-v1'] : ['whisper-1']
    };
  }

  /**
   * Test OpenAI connection
   */
  async testConnection(): Promise<boolean> {
    if (this.useMock || !this.openai) {
      return false;
    }

    try {
      // Test with a small buffer
      const testBuffer = Buffer.from('test');
      const file = this.bufferToFile(testBuffer, 'audio/wav');
      
      await this.openai.audio.transcriptions.create({
        file: file,
        model: 'whisper-1',
      });
      
      return true;
    } catch (error: unknown) {
      const message = getErrorMessage(error);
      console.error('OpenAI connection test failed:', message);
      return false;
    }
  }
}

export const asrService = new ASRService();
